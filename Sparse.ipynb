{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All the packages that will be needed for natural language processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from spacy.lang.en import English\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import preprocessor as prepro # text prepro\n",
    "import tqdm #progress bar\n",
    "from gensim.models import LdaModel, CoherenceModel\n",
    "from gensim import corpora\n",
    "import seaborn as sns\n",
    "from imblearn.over_sampling import SMOTE\n",
    "SMOTE = SMOTE()\n",
    "import spacy #spacy for quick language prepro\n",
    "nlp = spacy.load('en_core_web_sm') #instantiating English module\n",
    "import scipy.sparse as ss\n",
    "\n",
    "# sampling, splitting\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# loading ML libraries\n",
    "from sklearn.pipeline import make_pipeline #pipeline creation\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer #transforms text to sparse matrix\n",
    "from sklearn.linear_model import LogisticRegression #Logit model\n",
    "from sklearn.metrics import classification_report #that's self explanatory\n",
    "from sklearn.decomposition import TruncatedSVD #dimensionality reduction\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import altair as alt #viz\n",
    "\n",
    "#explainability\n",
    "import eli5\n",
    "from eli5.lime import TextExplainer\n",
    "\n",
    "# topic modeling\n",
    "\n",
    "from gensim.corpora.dictionary import Dictionary # Import the dictionary builder\n",
    "from gensim.models import LdaMulticore # we'll use the faster multicore version of LDA\n",
    "\n",
    "# Import pyLDAvis\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "\n",
    "%matplotlib inline\n",
    "pyLDAvis.enable_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepro settings\n",
    "# This is prob not relevant, since it has to do with tweets? \n",
    "# prepro.set_options(prepro.OPT.URL, prepro.OPT.NUMBER, prepro.OPT.RESERVED, prepro.OPT.MENTION, prepro.OPT.SMILEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading and appending the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "scopus2022 = pd.read_csv('scopus 2022 2021.csv',  sep = ',')\n",
    "scopus = pd.read_csv('scopus.csv',  sep = ',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maart\\AppData\\Local\\Temp\\ipykernel_39604\\3402760882.py:1: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data = scopus2022.append(scopus, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "data = scopus2022.append(scopus, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Authors</th>\n",
       "      <th>Author(s) ID</th>\n",
       "      <th>Title</th>\n",
       "      <th>Year</th>\n",
       "      <th>Source title</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Issue</th>\n",
       "      <th>Art. No.</th>\n",
       "      <th>Page start</th>\n",
       "      <th>Page end</th>\n",
       "      <th>...</th>\n",
       "      <th>ISBN</th>\n",
       "      <th>CODEN</th>\n",
       "      <th>PubMed ID</th>\n",
       "      <th>Language of Original Document</th>\n",
       "      <th>Abbreviated Source Title</th>\n",
       "      <th>Document Type</th>\n",
       "      <th>Publication Stage</th>\n",
       "      <th>Open Access</th>\n",
       "      <th>Source</th>\n",
       "      <th>EID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Yang T., Zhang X.</td>\n",
       "      <td>57907798100;56342888200;</td>\n",
       "      <td>FinTech adoption and financial inclusion: Evid...</td>\n",
       "      <td>2022</td>\n",
       "      <td>Journal of Banking and Finance</td>\n",
       "      <td>145</td>\n",
       "      <td>NaN</td>\n",
       "      <td>106668</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>JBFID</td>\n",
       "      <td>NaN</td>\n",
       "      <td>English</td>\n",
       "      <td>J. Bank. Financ.</td>\n",
       "      <td>Article</td>\n",
       "      <td>Final</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Scopus</td>\n",
       "      <td>2-s2.0-85138806241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Wang X., Wang Y., Zhao Y.</td>\n",
       "      <td>57193015825;57901727900;57901783600;</td>\n",
       "      <td>Financial permeation and rural poverty reducti...</td>\n",
       "      <td>2022</td>\n",
       "      <td>China Economic Review</td>\n",
       "      <td>76</td>\n",
       "      <td>NaN</td>\n",
       "      <td>101863</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>English</td>\n",
       "      <td>China Econ. Rev.</td>\n",
       "      <td>Article</td>\n",
       "      <td>Final</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Scopus</td>\n",
       "      <td>2-s2.0-85138589769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Dzandu M.D., Hanu C., Amegbe H.</td>\n",
       "      <td>56590001600;57201152816;57194904537;</td>\n",
       "      <td>Gamification of mobile money payment for gener...</td>\n",
       "      <td>2022</td>\n",
       "      <td>Technological Forecasting and Social Change</td>\n",
       "      <td>185</td>\n",
       "      <td>NaN</td>\n",
       "      <td>122049</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>English</td>\n",
       "      <td>Technol. Forecast. Soc. Change</td>\n",
       "      <td>Article</td>\n",
       "      <td>Final</td>\n",
       "      <td>All Open Access, Hybrid Gold, Green</td>\n",
       "      <td>Scopus</td>\n",
       "      <td>2-s2.0-85138450268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Grassi L., Fantaccini S.</td>\n",
       "      <td>57192656409;57895835500;</td>\n",
       "      <td>An overview of Fintech applications to solve t...</td>\n",
       "      <td>2022</td>\n",
       "      <td>Financial Innovation</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>84</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>English</td>\n",
       "      <td>Financial Innov.</td>\n",
       "      <td>Article</td>\n",
       "      <td>Final</td>\n",
       "      <td>All Open Access, Gold, Green</td>\n",
       "      <td>Scopus</td>\n",
       "      <td>2-s2.0-85138286241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DeFusco A.A., Tang H., Yannelis C.</td>\n",
       "      <td>57193852071;57890330600;55413678100;</td>\n",
       "      <td>Measuring the welfare cost of asymmetric infor...</td>\n",
       "      <td>2022</td>\n",
       "      <td>Journal of Financial Economics</td>\n",
       "      <td>146</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>821</td>\n",
       "      <td>840.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>JFECD</td>\n",
       "      <td>NaN</td>\n",
       "      <td>English</td>\n",
       "      <td>J. Financ. Econ.</td>\n",
       "      <td>Article</td>\n",
       "      <td>Final</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Scopus</td>\n",
       "      <td>2-s2.0-85138101180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3395</th>\n",
       "      <td>Fayard E.H.</td>\n",
       "      <td>35072693600;</td>\n",
       "      <td>ACC pressure cleaning</td>\n",
       "      <td>2010</td>\n",
       "      <td>Power Engineering (Barrington, Illinois)</td>\n",
       "      <td>114</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>POENA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>English</td>\n",
       "      <td>Power Eng. Barrington Ill</td>\n",
       "      <td>Short Survey</td>\n",
       "      <td>Final</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Scopus</td>\n",
       "      <td>2-s2.0-77953829082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3396</th>\n",
       "      <td>Dassanayake M.M.K., Tilakarathne C.</td>\n",
       "      <td>57221353725;55762978100;</td>\n",
       "      <td>Predicting trading signals of Sri Lankan stock...</td>\n",
       "      <td>2010</td>\n",
       "      <td>Technological Developments in Networking, Educ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>269</td>\n",
       "      <td>273</td>\n",
       "      <td>...</td>\n",
       "      <td>9789048191505</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>English</td>\n",
       "      <td>Technol. Dev. Networking, Educ. Autom.</td>\n",
       "      <td>Conference Paper</td>\n",
       "      <td>Final</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Scopus</td>\n",
       "      <td>2-s2.0-84878897261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3397</th>\n",
       "      <td>Hao H.-N.</td>\n",
       "      <td>36617357100;</td>\n",
       "      <td>Notice of Retraction: Short-term forecasting o...</td>\n",
       "      <td>2010</td>\n",
       "      <td>Proceedings - 2010 6th International Conferenc...</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5584528</td>\n",
       "      <td>1838</td>\n",
       "      <td>1841</td>\n",
       "      <td>...</td>\n",
       "      <td>9781424459612</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>English</td>\n",
       "      <td>Proc. - Int. Conf. Nat. Comput., ICNC</td>\n",
       "      <td>Retracted</td>\n",
       "      <td>Final</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Scopus</td>\n",
       "      <td>2-s2.0-78149350510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3398</th>\n",
       "      <td>Ma Z.-X., Zhang W.</td>\n",
       "      <td>55479146300;56621528900;</td>\n",
       "      <td>Notice of Retraction: An discrimination resear...</td>\n",
       "      <td>2010</td>\n",
       "      <td>ICAMS 2010 - Proceedings of 2010 IEEE Internat...</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5553273</td>\n",
       "      <td>116</td>\n",
       "      <td>119</td>\n",
       "      <td>...</td>\n",
       "      <td>9781424469291</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>English</td>\n",
       "      <td>ICAMS - Proc. IEEE Int. Conf. Adv. Manage. Sci.</td>\n",
       "      <td>Retracted</td>\n",
       "      <td>Final</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Scopus</td>\n",
       "      <td>2-s2.0-77957273781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3399</th>\n",
       "      <td>Danial S.N., Noor S.R., Usmani B.A., Zaidi S.J.H.</td>\n",
       "      <td>24823887800;24825002200;26666352700;57549810600;</td>\n",
       "      <td>A dynamical system and neural network perspect...</td>\n",
       "      <td>2008</td>\n",
       "      <td>Communications in Computer and Information Sci...</td>\n",
       "      <td>20 CCIS</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>88</td>\n",
       "      <td>99</td>\n",
       "      <td>...</td>\n",
       "      <td>3540898522; 9783540898528</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>English</td>\n",
       "      <td>Commun. Comput. Info. Sci.</td>\n",
       "      <td>Conference Paper</td>\n",
       "      <td>Final</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Scopus</td>\n",
       "      <td>2-s2.0-85099426338</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3400 rows × 54 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Authors  \\\n",
       "0                                     Yang T., Zhang X.   \n",
       "1                             Wang X., Wang Y., Zhao Y.   \n",
       "2                       Dzandu M.D., Hanu C., Amegbe H.   \n",
       "3                              Grassi L., Fantaccini S.   \n",
       "4                    DeFusco A.A., Tang H., Yannelis C.   \n",
       "...                                                 ...   \n",
       "3395                                        Fayard E.H.   \n",
       "3396                Dassanayake M.M.K., Tilakarathne C.   \n",
       "3397                                          Hao H.-N.   \n",
       "3398                                 Ma Z.-X., Zhang W.   \n",
       "3399  Danial S.N., Noor S.R., Usmani B.A., Zaidi S.J.H.   \n",
       "\n",
       "                                          Author(s) ID  \\\n",
       "0                             57907798100;56342888200;   \n",
       "1                 57193015825;57901727900;57901783600;   \n",
       "2                 56590001600;57201152816;57194904537;   \n",
       "3                             57192656409;57895835500;   \n",
       "4                 57193852071;57890330600;55413678100;   \n",
       "...                                                ...   \n",
       "3395                                      35072693600;   \n",
       "3396                          57221353725;55762978100;   \n",
       "3397                                      36617357100;   \n",
       "3398                          55479146300;56621528900;   \n",
       "3399  24823887800;24825002200;26666352700;57549810600;   \n",
       "\n",
       "                                                  Title  Year  \\\n",
       "0     FinTech adoption and financial inclusion: Evid...  2022   \n",
       "1     Financial permeation and rural poverty reducti...  2022   \n",
       "2     Gamification of mobile money payment for gener...  2022   \n",
       "3     An overview of Fintech applications to solve t...  2022   \n",
       "4     Measuring the welfare cost of asymmetric infor...  2022   \n",
       "...                                                 ...   ...   \n",
       "3395                              ACC pressure cleaning  2010   \n",
       "3396  Predicting trading signals of Sri Lankan stock...  2010   \n",
       "3397  Notice of Retraction: Short-term forecasting o...  2010   \n",
       "3398  Notice of Retraction: An discrimination resear...  2010   \n",
       "3399  A dynamical system and neural network perspect...  2008   \n",
       "\n",
       "                                           Source title   Volume Issue  \\\n",
       "0                        Journal of Banking and Finance      145   NaN   \n",
       "1                                 China Economic Review       76   NaN   \n",
       "2           Technological Forecasting and Social Change      185   NaN   \n",
       "3                                  Financial Innovation        8     1   \n",
       "4                        Journal of Financial Economics      146     3   \n",
       "...                                                 ...      ...   ...   \n",
       "3395           Power Engineering (Barrington, Illinois)      114     5   \n",
       "3396  Technological Developments in Networking, Educ...      NaN   NaN   \n",
       "3397  Proceedings - 2010 6th International Conferenc...        4   NaN   \n",
       "3398  ICAMS 2010 - Proceedings of 2010 IEEE Internat...        3   NaN   \n",
       "3399  Communications in Computer and Information Sci...  20 CCIS   NaN   \n",
       "\n",
       "     Art. No. Page start Page end  ...                       ISBN  CODEN  \\\n",
       "0      106668        NaN      NaN  ...                        NaN  JBFID   \n",
       "1      101863        NaN      NaN  ...                        NaN    NaN   \n",
       "2      122049        NaN      NaN  ...                        NaN    NaN   \n",
       "3          84        NaN      NaN  ...                        NaN    NaN   \n",
       "4         NaN        821    840.0  ...                        NaN  JFECD   \n",
       "...       ...        ...      ...  ...                        ...    ...   \n",
       "3395      NaN         22      NaN  ...                        NaN  POENA   \n",
       "3396      NaN        269      273  ...              9789048191505    NaN   \n",
       "3397  5584528       1838     1841  ...              9781424459612    NaN   \n",
       "3398  5553273        116      119  ...              9781424469291    NaN   \n",
       "3399      NaN         88       99  ...  3540898522; 9783540898528    NaN   \n",
       "\n",
       "     PubMed ID Language of Original Document  \\\n",
       "0          NaN                       English   \n",
       "1          NaN                       English   \n",
       "2          NaN                       English   \n",
       "3          NaN                       English   \n",
       "4          NaN                       English   \n",
       "...        ...                           ...   \n",
       "3395       NaN                       English   \n",
       "3396       NaN                       English   \n",
       "3397       NaN                       English   \n",
       "3398       NaN                       English   \n",
       "3399       NaN                       English   \n",
       "\n",
       "                             Abbreviated Source Title     Document Type  \\\n",
       "0                                    J. Bank. Financ.           Article   \n",
       "1                                    China Econ. Rev.           Article   \n",
       "2                      Technol. Forecast. Soc. Change           Article   \n",
       "3                                    Financial Innov.           Article   \n",
       "4                                    J. Financ. Econ.           Article   \n",
       "...                                               ...               ...   \n",
       "3395                        Power Eng. Barrington Ill      Short Survey   \n",
       "3396           Technol. Dev. Networking, Educ. Autom.  Conference Paper   \n",
       "3397            Proc. - Int. Conf. Nat. Comput., ICNC         Retracted   \n",
       "3398  ICAMS - Proc. IEEE Int. Conf. Adv. Manage. Sci.         Retracted   \n",
       "3399                       Commun. Comput. Info. Sci.  Conference Paper   \n",
       "\n",
       "     Publication Stage                          Open Access  Source  \\\n",
       "0                Final                                  NaN  Scopus   \n",
       "1                Final                                  NaN  Scopus   \n",
       "2                Final  All Open Access, Hybrid Gold, Green  Scopus   \n",
       "3                Final         All Open Access, Gold, Green  Scopus   \n",
       "4                Final                                  NaN  Scopus   \n",
       "...                ...                                  ...     ...   \n",
       "3395             Final                                  NaN  Scopus   \n",
       "3396             Final                                  NaN  Scopus   \n",
       "3397             Final                                  NaN  Scopus   \n",
       "3398             Final                                  NaN  Scopus   \n",
       "3399             Final                                  NaN  Scopus   \n",
       "\n",
       "                     EID  \n",
       "0     2-s2.0-85138806241  \n",
       "1     2-s2.0-85138589769  \n",
       "2     2-s2.0-85138450268  \n",
       "3     2-s2.0-85138286241  \n",
       "4     2-s2.0-85138101180  \n",
       "...                  ...  \n",
       "3395  2-s2.0-77953829082  \n",
       "3396  2-s2.0-84878897261  \n",
       "3397  2-s2.0-78149350510  \n",
       "3398  2-s2.0-77957273781  \n",
       "3399  2-s2.0-85099426338  \n",
       "\n",
       "[3400 rows x 54 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3400 entries, 0 to 3399\n",
      "Data columns (total 54 columns):\n",
      " #   Column                         Non-Null Count  Dtype  \n",
      "---  ------                         --------------  -----  \n",
      " 0   Authors                        3400 non-null   object \n",
      " 1   Author(s) ID                   3399 non-null   object \n",
      " 2   Title                          3400 non-null   object \n",
      " 3   Year                           3400 non-null   int64  \n",
      " 4   Source title                   3400 non-null   object \n",
      " 5   Volume                         2320 non-null   object \n",
      " 6   Issue                          1426 non-null   object \n",
      " 7   Art. No.                       1051 non-null   object \n",
      " 8   Page start                     2336 non-null   object \n",
      " 9   Page end                       2326 non-null   object \n",
      " 10  Page count                     38 non-null     float64\n",
      " 11  Cited by                       2164 non-null   float64\n",
      " 12  DOI                            2972 non-null   object \n",
      " 13  Link                           3400 non-null   object \n",
      " 14  Affiliations                   3305 non-null   object \n",
      " 15  Authors with affiliations      3342 non-null   object \n",
      " 16  Abstract                       3400 non-null   object \n",
      " 17  Author Keywords                2781 non-null   object \n",
      " 18  Index Keywords                 1616 non-null   object \n",
      " 19  Molecular Sequence Numbers     0 non-null      float64\n",
      " 20  Chemicals/CAS                  4 non-null      object \n",
      " 21  Tradenames                     0 non-null      float64\n",
      " 22  Manufacturers                  0 non-null      float64\n",
      " 23  Funding Details                827 non-null    object \n",
      " 24  Funding Text 1                 1039 non-null   object \n",
      " 25  Funding Text 2                 141 non-null    object \n",
      " 26  Funding Text 3                 7 non-null      object \n",
      " 27  Funding Text 4                 1 non-null      object \n",
      " 28  Funding Text 5                 1 non-null      object \n",
      " 29  Funding Text 6                 1 non-null      object \n",
      " 30  Funding Text 7                 1 non-null      object \n",
      " 31  Funding Text 8                 0 non-null      float64\n",
      " 32  Funding Text 9                 0 non-null      float64\n",
      " 33  Funding Text 10                0 non-null      float64\n",
      " 34  References                     3212 non-null   object \n",
      " 35  Correspondence Address         2354 non-null   object \n",
      " 36  Editors                        491 non-null    object \n",
      " 37  Sponsors                       319 non-null    object \n",
      " 38  Publisher                      3394 non-null   object \n",
      " 39  Conference name                1063 non-null   object \n",
      " 40  Conference date                1062 non-null   object \n",
      " 41  Conference location            3 non-null      object \n",
      " 42  Conference code                1065 non-null   float64\n",
      " 43  ISSN                           2540 non-null   object \n",
      " 44  ISBN                           1174 non-null   object \n",
      " 45  CODEN                          276 non-null    object \n",
      " 46  PubMed ID                      36 non-null     float64\n",
      " 47  Language of Original Document  3400 non-null   object \n",
      " 48  Abbreviated Source Title       3396 non-null   object \n",
      " 49  Document Type                  3400 non-null   object \n",
      " 50  Publication Stage              3400 non-null   object \n",
      " 51  Open Access                    1097 non-null   object \n",
      " 52  Source                         3400 non-null   object \n",
      " 53  EID                            3400 non-null   object \n",
      "dtypes: float64(10), int64(1), object(43)\n",
      "memory usage: 1.4+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Authors</th>\n",
       "      <th>Author(s) ID</th>\n",
       "      <th>Title</th>\n",
       "      <th>Source title</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Issue</th>\n",
       "      <th>Art. No.</th>\n",
       "      <th>Page start</th>\n",
       "      <th>Page end</th>\n",
       "      <th>DOI</th>\n",
       "      <th>...</th>\n",
       "      <th>ISSN</th>\n",
       "      <th>ISBN</th>\n",
       "      <th>CODEN</th>\n",
       "      <th>Language of Original Document</th>\n",
       "      <th>Abbreviated Source Title</th>\n",
       "      <th>Document Type</th>\n",
       "      <th>Publication Stage</th>\n",
       "      <th>Open Access</th>\n",
       "      <th>Source</th>\n",
       "      <th>EID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>3400</td>\n",
       "      <td>3399</td>\n",
       "      <td>3400</td>\n",
       "      <td>3400</td>\n",
       "      <td>2320</td>\n",
       "      <td>1426</td>\n",
       "      <td>1051</td>\n",
       "      <td>2336</td>\n",
       "      <td>2326</td>\n",
       "      <td>2972</td>\n",
       "      <td>...</td>\n",
       "      <td>2540</td>\n",
       "      <td>1174</td>\n",
       "      <td>276</td>\n",
       "      <td>3400</td>\n",
       "      <td>3396</td>\n",
       "      <td>3400</td>\n",
       "      <td>3400</td>\n",
       "      <td>1097</td>\n",
       "      <td>3400</td>\n",
       "      <td>3400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>3167</td>\n",
       "      <td>3156</td>\n",
       "      <td>3389</td>\n",
       "      <td>1455</td>\n",
       "      <td>495</td>\n",
       "      <td>81</td>\n",
       "      <td>1023</td>\n",
       "      <td>1013</td>\n",
       "      <td>1369</td>\n",
       "      <td>2969</td>\n",
       "      <td>...</td>\n",
       "      <td>949</td>\n",
       "      <td>821</td>\n",
       "      <td>124</td>\n",
       "      <td>4</td>\n",
       "      <td>1429</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>3400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>[No author name available]</td>\n",
       "      <td>[No author id available]</td>\n",
       "      <td>Introduction</td>\n",
       "      <td>ACM International Conference Proceeding Series</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>10.1142/9789811235825_0003</td>\n",
       "      <td>...</td>\n",
       "      <td>21945357</td>\n",
       "      <td>9781799832591; 9781799832577</td>\n",
       "      <td>JEBUD</td>\n",
       "      <td>English</td>\n",
       "      <td>ACM Int. Conf. Proc. Ser.</td>\n",
       "      <td>Article</td>\n",
       "      <td>Final</td>\n",
       "      <td>All Open Access, Gold</td>\n",
       "      <td>Scopus</td>\n",
       "      <td>2-s2.0-85138806241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>58</td>\n",
       "      <td>58</td>\n",
       "      <td>4</td>\n",
       "      <td>93</td>\n",
       "      <td>95</td>\n",
       "      <td>322</td>\n",
       "      <td>4</td>\n",
       "      <td>158</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>53</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>3395</td>\n",
       "      <td>93</td>\n",
       "      <td>1770</td>\n",
       "      <td>3241</td>\n",
       "      <td>278</td>\n",
       "      <td>3400</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows × 43 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Authors              Author(s) ID         Title  \\\n",
       "count                         3400                      3399          3400   \n",
       "unique                        3167                      3156          3389   \n",
       "top     [No author name available]  [No author id available]  Introduction   \n",
       "freq                            58                        58             4   \n",
       "\n",
       "                                          Source title Volume Issue Art. No.  \\\n",
       "count                                             3400   2320  1426     1051   \n",
       "unique                                            1455    495    81     1023   \n",
       "top     ACM International Conference Proceeding Series      8     1        8   \n",
       "freq                                                93     95   322        4   \n",
       "\n",
       "       Page start Page end                         DOI  ...      ISSN  \\\n",
       "count        2336     2326                        2972  ...      2540   \n",
       "unique       1013     1369                        2969  ...       949   \n",
       "top             1       20  10.1142/9789811235825_0003  ...  21945357   \n",
       "freq          158       11                           2  ...        53   \n",
       "\n",
       "                                ISBN  CODEN Language of Original Document  \\\n",
       "count                           1174    276                          3400   \n",
       "unique                           821    124                             4   \n",
       "top     9781799832591; 9781799832577  JEBUD                       English   \n",
       "freq                              15     15                          3395   \n",
       "\n",
       "         Abbreviated Source Title Document Type Publication Stage  \\\n",
       "count                        3396          3400              3400   \n",
       "unique                       1429            13                 2   \n",
       "top     ACM Int. Conf. Proc. Ser.       Article             Final   \n",
       "freq                           93          1770              3241   \n",
       "\n",
       "                  Open Access  Source                 EID  \n",
       "count                    1097    3400                3400  \n",
       "unique                      7       1                3400  \n",
       "top     All Open Access, Gold  Scopus  2-s2.0-85138806241  \n",
       "freq                      278    3400                   1  \n",
       "\n",
       "[4 rows x 43 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe(include='object')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       Consumption; Consumption inequality; Credit co...\n",
       "1       Bank branch expansion; Financial permeation; F...\n",
       "2       Customer value; FinTech; Gamification; Marketi...\n",
       "3       Crowdfunding; Fintech; Health crowdfunding; He...\n",
       "4       Asymmetric information; Consumer credit; Exper...\n",
       "                              ...                        \n",
       "3395                                                  NaN\n",
       "3396                                                  NaN\n",
       "3397    Genetic-neural network; Short-term forecasting...\n",
       "3398    Discrimination analysis; Insider trading; Mark...\n",
       "3399    correlation dimension; KSE-100 index returns; ...\n",
       "Name: Author Keywords, Length: 3400, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Author Keywords']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[['Authors', 'Author(s) ID','Title', 'Abstract','Year', 'Source title', 'Author Keywords']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.series.Series'>\n",
      "RangeIndex: 3400 entries, 0 to 3399\n",
      "Series name: Author Keywords\n",
      "Non-Null Count  Dtype \n",
      "--------------  ----- \n",
      "2781 non-null   object\n",
      "dtypes: object(1)\n",
      "memory usage: 26.7+ KB\n"
     ]
    }
   ],
   "source": [
    "data['Author Keywords'].info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data['Author Keywords'] = data['Author Keywords'].astype({'Author Keywords':'string'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       Consumption; Consumption inequality; Credit co...\n",
       "1       Bank branch expansion; Financial permeation; F...\n",
       "2       Customer value; FinTech; Gamification; Marketi...\n",
       "3       Crowdfunding; Fintech; Health crowdfunding; He...\n",
       "4       Asymmetric information; Consumer credit; Exper...\n",
       "                              ...                        \n",
       "3391    Aerodynamic Derivatives; Lateral-directional; ...\n",
       "3393    DXNN; Evolutionary computation; Financial anal...\n",
       "3397    Genetic-neural network; Short-term forecasting...\n",
       "3398    Discrimination analysis; Insider trading; Mark...\n",
       "3399    correlation dimension; KSE-100 index returns; ...\n",
       "Name: Author Keywords, Length: 2781, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Author Keywords'].dropna(how='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Consumption; Consumption inequality; Credit co...\n",
       "1    Bank branch expansion; Financial permeation; F...\n",
       "2    Customer value; FinTech; Gamification; Marketi...\n",
       "3    Crowdfunding; Fintech; Health crowdfunding; He...\n",
       "4    Asymmetric information; Consumer credit; Exper...\n",
       "Name: Author Keywords, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Author Keywords'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    2781\n",
       "True      619\n",
       "Name: Author Keywords, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Author Keywords'].isnull().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3400 [00:00<?, ?it/s]"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "[E1041] Expected a string, Doc, or bytes as input, but got: <class 'float'>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\maart\\OneDrive\\Skrivebord\\M2\\Module-2\\Sparse.ipynb Cell 18\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/maart/OneDrive/Skrivebord/M2/Module-2/Sparse.ipynb#X22sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m clean_text \u001b[39m=\u001b[39m []\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/maart/OneDrive/Skrivebord/M2/Module-2/Sparse.ipynb#X22sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m pbar \u001b[39m=\u001b[39m tqdm\u001b[39m.\u001b[39mtqdm(total\u001b[39m=\u001b[39m\u001b[39mlen\u001b[39m(data[\u001b[39m'\u001b[39m\u001b[39mAuthor Keywords\u001b[39m\u001b[39m'\u001b[39m]),position\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m, leave\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/maart/OneDrive/Skrivebord/M2/Module-2/Sparse.ipynb#X22sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mfor\u001b[39;00m text \u001b[39min\u001b[39;00m nlp\u001b[39m.\u001b[39mpipe(data[\u001b[39m'\u001b[39m\u001b[39mAuthor Keywords\u001b[39m\u001b[39m'\u001b[39m], disable\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39mtagger\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mparser\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mner\u001b[39m\u001b[39m\"\u001b[39m]):\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/maart/OneDrive/Skrivebord/M2/Module-2/Sparse.ipynb#X22sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m   txt \u001b[39m=\u001b[39m [token\u001b[39m.\u001b[39mlemma_\u001b[39m.\u001b[39mlower() \u001b[39mfor\u001b[39;00m token \u001b[39min\u001b[39;00m text \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/maart/OneDrive/Skrivebord/M2/Module-2/Sparse.ipynb#X22sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m          \u001b[39mif\u001b[39;00m token\u001b[39m.\u001b[39mis_alpha \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/maart/OneDrive/Skrivebord/M2/Module-2/Sparse.ipynb#X22sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m          \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m token\u001b[39m.\u001b[39mis_stop \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/maart/OneDrive/Skrivebord/M2/Module-2/Sparse.ipynb#X22sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m          \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m token\u001b[39m.\u001b[39mis_punct]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/maart/OneDrive/Skrivebord/M2/Module-2/Sparse.ipynb#X22sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m   clean_text\u001b[39m.\u001b[39mappend(\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(txt))\n",
      "File \u001b[1;32mc:\\Users\\maart\\anaconda3\\lib\\site-packages\\spacy\\language.py:1589\u001b[0m, in \u001b[0;36mLanguage.pipe\u001b[1;34m(self, texts, as_tuples, batch_size, disable, component_cfg, n_process)\u001b[0m\n\u001b[0;32m   1587\u001b[0m     \u001b[39mfor\u001b[39;00m pipe \u001b[39min\u001b[39;00m pipes:\n\u001b[0;32m   1588\u001b[0m         docs \u001b[39m=\u001b[39m pipe(docs)\n\u001b[1;32m-> 1589\u001b[0m \u001b[39mfor\u001b[39;00m doc \u001b[39min\u001b[39;00m docs:\n\u001b[0;32m   1590\u001b[0m     \u001b[39myield\u001b[39;00m doc\n",
      "File \u001b[1;32mc:\\Users\\maart\\anaconda3\\lib\\site-packages\\spacy\\util.py:1651\u001b[0m, in \u001b[0;36m_pipe\u001b[1;34m(docs, proc, name, default_error_handler, kwargs)\u001b[0m\n\u001b[0;32m   1643\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_pipe\u001b[39m(\n\u001b[0;32m   1644\u001b[0m     docs: Iterable[\u001b[39m\"\u001b[39m\u001b[39mDoc\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[0;32m   1645\u001b[0m     proc: \u001b[39m\"\u001b[39m\u001b[39mPipe\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1648\u001b[0m     kwargs: Mapping[\u001b[39mstr\u001b[39m, Any],\n\u001b[0;32m   1649\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Iterator[\u001b[39m\"\u001b[39m\u001b[39mDoc\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[0;32m   1650\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(proc, \u001b[39m\"\u001b[39m\u001b[39mpipe\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m-> 1651\u001b[0m         \u001b[39myield from\u001b[39;00m proc\u001b[39m.\u001b[39mpipe(docs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1652\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1653\u001b[0m         \u001b[39m# We added some args for pipe that __call__ doesn't expect.\u001b[39;00m\n\u001b[0;32m   1654\u001b[0m         kwargs \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(kwargs)\n",
      "File \u001b[1;32mc:\\Users\\maart\\anaconda3\\lib\\site-packages\\spacy\\pipeline\\pipe.pyx:53\u001b[0m, in \u001b[0;36mpipe\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\maart\\anaconda3\\lib\\site-packages\\spacy\\util.py:1651\u001b[0m, in \u001b[0;36m_pipe\u001b[1;34m(docs, proc, name, default_error_handler, kwargs)\u001b[0m\n\u001b[0;32m   1643\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_pipe\u001b[39m(\n\u001b[0;32m   1644\u001b[0m     docs: Iterable[\u001b[39m\"\u001b[39m\u001b[39mDoc\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[0;32m   1645\u001b[0m     proc: \u001b[39m\"\u001b[39m\u001b[39mPipe\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1648\u001b[0m     kwargs: Mapping[\u001b[39mstr\u001b[39m, Any],\n\u001b[0;32m   1649\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Iterator[\u001b[39m\"\u001b[39m\u001b[39mDoc\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[0;32m   1650\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(proc, \u001b[39m\"\u001b[39m\u001b[39mpipe\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m-> 1651\u001b[0m         \u001b[39myield from\u001b[39;00m proc\u001b[39m.\u001b[39mpipe(docs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1652\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1653\u001b[0m         \u001b[39m# We added some args for pipe that __call__ doesn't expect.\u001b[39;00m\n\u001b[0;32m   1654\u001b[0m         kwargs \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(kwargs)\n",
      "File \u001b[1;32mc:\\Users\\maart\\anaconda3\\lib\\site-packages\\spacy\\pipeline\\pipe.pyx:53\u001b[0m, in \u001b[0;36mpipe\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\maart\\anaconda3\\lib\\site-packages\\spacy\\util.py:1651\u001b[0m, in \u001b[0;36m_pipe\u001b[1;34m(docs, proc, name, default_error_handler, kwargs)\u001b[0m\n\u001b[0;32m   1643\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_pipe\u001b[39m(\n\u001b[0;32m   1644\u001b[0m     docs: Iterable[\u001b[39m\"\u001b[39m\u001b[39mDoc\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[0;32m   1645\u001b[0m     proc: \u001b[39m\"\u001b[39m\u001b[39mPipe\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1648\u001b[0m     kwargs: Mapping[\u001b[39mstr\u001b[39m, Any],\n\u001b[0;32m   1649\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Iterator[\u001b[39m\"\u001b[39m\u001b[39mDoc\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[0;32m   1650\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(proc, \u001b[39m\"\u001b[39m\u001b[39mpipe\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m-> 1651\u001b[0m         \u001b[39myield from\u001b[39;00m proc\u001b[39m.\u001b[39mpipe(docs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1652\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1653\u001b[0m         \u001b[39m# We added some args for pipe that __call__ doesn't expect.\u001b[39;00m\n\u001b[0;32m   1654\u001b[0m         kwargs \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(kwargs)\n",
      "File \u001b[1;32mc:\\Users\\maart\\anaconda3\\lib\\site-packages\\spacy\\pipeline\\trainable_pipe.pyx:73\u001b[0m, in \u001b[0;36mpipe\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\maart\\anaconda3\\lib\\site-packages\\spacy\\util.py:1600\u001b[0m, in \u001b[0;36mminibatch\u001b[1;34m(items, size)\u001b[0m\n\u001b[0;32m   1598\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m   1599\u001b[0m     batch_size \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39m(size_)\n\u001b[1;32m-> 1600\u001b[0m     batch \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39;49m(itertools\u001b[39m.\u001b[39;49mislice(items, \u001b[39mint\u001b[39;49m(batch_size)))\n\u001b[0;32m   1601\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(batch) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m   1602\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\maart\\anaconda3\\lib\\site-packages\\spacy\\language.py:1586\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   1583\u001b[0m     docs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_multiprocessing_pipe(texts, pipes, n_process, batch_size)\n\u001b[0;32m   1584\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1585\u001b[0m     \u001b[39m# if n_process == 1, no processes are forked.\u001b[39;00m\n\u001b[1;32m-> 1586\u001b[0m     docs \u001b[39m=\u001b[39m (\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_ensure_doc(text) \u001b[39mfor\u001b[39;00m text \u001b[39min\u001b[39;00m texts)\n\u001b[0;32m   1587\u001b[0m     \u001b[39mfor\u001b[39;00m pipe \u001b[39min\u001b[39;00m pipes:\n\u001b[0;32m   1588\u001b[0m         docs \u001b[39m=\u001b[39m pipe(docs)\n",
      "File \u001b[1;32mc:\\Users\\maart\\anaconda3\\lib\\site-packages\\spacy\\language.py:1108\u001b[0m, in \u001b[0;36mLanguage._ensure_doc\u001b[1;34m(self, doc_like)\u001b[0m\n\u001b[0;32m   1106\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(doc_like, \u001b[39mbytes\u001b[39m):\n\u001b[0;32m   1107\u001b[0m     \u001b[39mreturn\u001b[39;00m Doc(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvocab)\u001b[39m.\u001b[39mfrom_bytes(doc_like)\n\u001b[1;32m-> 1108\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(Errors\u001b[39m.\u001b[39mE1041\u001b[39m.\u001b[39mformat(\u001b[39mtype\u001b[39m\u001b[39m=\u001b[39m\u001b[39mtype\u001b[39m(doc_like)))\n",
      "\u001b[1;31mValueError\u001b[0m: [E1041] Expected a string, Doc, or bytes as input, but got: <class 'float'>"
     ]
    }
   ],
   "source": [
    "# run progress bar and clean up using spacy but without some heavy parts of the pipeline\n",
    "\n",
    "clean_text = []\n",
    "\n",
    "pbar = tqdm.tqdm(total=len(data['Author Keywords']),position=0, leave=True)\n",
    "\n",
    "for text in nlp.pipe(data['Author Keywords'], disable=[\"tagger\", \"parser\", \"ner\"]):\n",
    "\n",
    "  txt = [token.lemma_.lower() for token in text \n",
    "         if token.is_alpha \n",
    "         and not token.is_stop \n",
    "         and not token.is_punct]\n",
    "\n",
    "  clean_text.append(\" \".join(txt))\n",
    "\n",
    "  pbar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write everything into one function that can be re-used later\n",
    "def text_prepro(texts):\n",
    "  \"\"\"\n",
    "  takes in a pandas series (1 column of a DF)\n",
    "  removes twitter stuff\n",
    "  lowercases, normalizes text\n",
    "  \"\"\"\n",
    "  texts_clean = texts.map(lambda t: prepro.clean(t))\n",
    "  clean_container = []\n",
    "\n",
    "  pbar = tqdm.tqdm(total=len(texts_clean),position=0, leave=True)\n",
    "\n",
    "  for text in nlp.pipe(texts_clean, disable=[\"tagger\", \"parser\", \"ner\"]):\n",
    "\n",
    "    txt = [token.lemma_.lower() for token in text \n",
    "          if token.is_alpha \n",
    "          and not token.is_stop \n",
    "          and not token.is_punct]\n",
    "\n",
    "    clean_container.append(\" \".join(txt))\n",
    "    pbar.update(1)\n",
    "  \n",
    "  return clean_container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply all prepro-pipeline to texts\n",
    "data['Author_Keywords'] = text_prepro(data['Author_Keywords'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess texts (we need tokens)\n",
    "tokens = []\n",
    "\n",
    "for summary in nlp.pipe(data['Author Keywords'], disable=[\"ner\"]):\n",
    "  proj_tok = [token.lemma_.lower() for token in summary \n",
    "              if token.pos_ in ['NOUN', 'PROPN', 'ADJ', 'ADV'] \n",
    "              and not token.is_stop\n",
    "              and not token.is_punct] \n",
    "  tokens.append(proj_tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['tokens'] = tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Dictionary from the articles: dictionary\n",
    "dictionary = Dictionary(data['tokens'])\n",
    "# filter out low-frequency / high-frequency stuff, also limit the vocabulary to max 500 words, since we have a low sample size ( In the example he did, he used 1000)\n",
    "dictionary.filter_extremes(no_below=50, no_above=0.5, keep_n=1000)\n",
    "# construct corpus using this dictionary\n",
    "corpus = [dictionary.doc2bow(doc) for doc in data['tokens']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have to scale and transform our data into numerical values since they are categorical values \n",
    "le_keywords = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maart\\AppData\\Local\\Temp\\ipykernel_39604\\1728858668.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['keywords_ID'] = le_keywords.fit_transform(data['Author Keywords'])\n"
     ]
    }
   ],
   "source": [
    "data['keywords_ID'] = le_keywords.fit_transform(data['Author Keywords'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "ones = np.ones(len(data), np.uint32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (2535259027.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Input \u001b[1;32mIn [38]\u001b[1;36m\u001b[0m\n\u001b[1;33m    matrix = ss.coo_matrix((ones, (data['keywords_ID']))\u001b[0m\n\u001b[1;37m                                                        ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "matrix = ss.coo_matrix((ones, (data['keywords_ID']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       1449\n",
       "1        550\n",
       "2       1623\n",
       "3       1567\n",
       "4        455\n",
       "        ... \n",
       "3395    2767\n",
       "3396    2767\n",
       "3397    2219\n",
       "3398    1829\n",
       "3399    2592\n",
       "Name: keywords_ID, Length: 3400, dtype: int32"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['keywords_ID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing UML packages\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Series' object has no attribute 'reshape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\maart\\OneDrive\\Skrivebord\\M2\\Module-2\\Sparse.ipynb Cell 28\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/maart/OneDrive/Skrivebord/M2/Module-2/Sparse.ipynb#Y142sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m data[\u001b[39m'\u001b[39m\u001b[39mkeywords_ID\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m data[\u001b[39m'\u001b[39;49m\u001b[39mkeywords_ID\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39;49mreshape(\u001b[39m1\u001b[39m,\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\maart\\anaconda3\\lib\\site-packages\\pandas\\core\\generic.py:5575\u001b[0m, in \u001b[0;36mNDFrame.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   5568\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[0;32m   5569\u001b[0m     name \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_internal_names_set\n\u001b[0;32m   5570\u001b[0m     \u001b[39mand\u001b[39;00m name \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_metadata\n\u001b[0;32m   5571\u001b[0m     \u001b[39mand\u001b[39;00m name \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_accessors\n\u001b[0;32m   5572\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_info_axis\u001b[39m.\u001b[39m_can_hold_identifiers_and_holds_name(name)\n\u001b[0;32m   5573\u001b[0m ):\n\u001b[0;32m   5574\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m[name]\n\u001b[1;32m-> 5575\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mobject\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m__getattribute__\u001b[39;49m(\u001b[39mself\u001b[39;49m, name)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Series' object has no attribute 'reshape'"
     ]
    }
   ],
   "source": [
    "data['keywords_ID'] = data['keywords_ID'].reshape(1,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected 2D array, got 1D array instead:\narray=[1449.  550. 1623. ... 2219. 1829. 2592.].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\maart\\OneDrive\\Skrivebord\\M2\\Module-2\\Sparse.ipynb Cell 28\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/maart/OneDrive/Skrivebord/M2/Module-2/Sparse.ipynb#X36sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m#Learn x-y relationships (principal components) and transform\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/maart/OneDrive/Skrivebord/M2/Module-2/Sparse.ipynb#X36sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m data_to_cluster_scaled \u001b[39m=\u001b[39m scaler\u001b[39m.\u001b[39;49mfit_transform(data[\u001b[39m'\u001b[39;49m\u001b[39mkeywords_ID\u001b[39;49m\u001b[39m'\u001b[39;49m])\n",
      "File \u001b[1;32mc:\\Users\\maart\\anaconda3\\lib\\site-packages\\sklearn\\base.py:867\u001b[0m, in \u001b[0;36mTransformerMixin.fit_transform\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    863\u001b[0m \u001b[39m# non-optimized default implementation; override when a better\u001b[39;00m\n\u001b[0;32m    864\u001b[0m \u001b[39m# method is possible for a given clustering algorithm\u001b[39;00m\n\u001b[0;32m    865\u001b[0m \u001b[39mif\u001b[39;00m y \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    866\u001b[0m     \u001b[39m# fit method of arity 1 (unsupervised transformation)\u001b[39;00m\n\u001b[1;32m--> 867\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfit(X, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\u001b[39m.\u001b[39mtransform(X)\n\u001b[0;32m    868\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    869\u001b[0m     \u001b[39m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfit(X, y, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\u001b[39m.\u001b[39mtransform(X)\n",
      "File \u001b[1;32mc:\\Users\\maart\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:809\u001b[0m, in \u001b[0;36mStandardScaler.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    807\u001b[0m \u001b[39m# Reset internal state before fitting\u001b[39;00m\n\u001b[0;32m    808\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()\n\u001b[1;32m--> 809\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpartial_fit(X, y, sample_weight)\n",
      "File \u001b[1;32mc:\\Users\\maart\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:844\u001b[0m, in \u001b[0;36mStandardScaler.partial_fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    812\u001b[0m \u001b[39m\"\"\"Online computation of mean and std on X for later scaling.\u001b[39;00m\n\u001b[0;32m    813\u001b[0m \n\u001b[0;32m    814\u001b[0m \u001b[39mAll of X is processed as a single batch. This is intended for cases\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    841\u001b[0m \u001b[39m    Fitted scaler.\u001b[39;00m\n\u001b[0;32m    842\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    843\u001b[0m first_call \u001b[39m=\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mn_samples_seen_\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 844\u001b[0m X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_data(\n\u001b[0;32m    845\u001b[0m     X,\n\u001b[0;32m    846\u001b[0m     accept_sparse\u001b[39m=\u001b[39;49m(\u001b[39m\"\u001b[39;49m\u001b[39mcsr\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mcsc\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[0;32m    847\u001b[0m     dtype\u001b[39m=\u001b[39;49mFLOAT_DTYPES,\n\u001b[0;32m    848\u001b[0m     force_all_finite\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mallow-nan\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    849\u001b[0m     reset\u001b[39m=\u001b[39;49mfirst_call,\n\u001b[0;32m    850\u001b[0m )\n\u001b[0;32m    851\u001b[0m n_features \u001b[39m=\u001b[39m X\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]\n\u001b[0;32m    853\u001b[0m \u001b[39mif\u001b[39;00m sample_weight \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\maart\\anaconda3\\lib\\site-packages\\sklearn\\base.py:577\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    575\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mValidation should be done on X, y or both.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    576\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m no_val_y:\n\u001b[1;32m--> 577\u001b[0m     X \u001b[39m=\u001b[39m check_array(X, input_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mX\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcheck_params)\n\u001b[0;32m    578\u001b[0m     out \u001b[39m=\u001b[39m X\n\u001b[0;32m    579\u001b[0m \u001b[39melif\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_y:\n",
      "File \u001b[1;32mc:\\Users\\maart\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:879\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    877\u001b[0m     \u001b[39m# If input is 1D raise error\u001b[39;00m\n\u001b[0;32m    878\u001b[0m     \u001b[39mif\u001b[39;00m array\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m--> 879\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    880\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mExpected 2D array, got 1D array instead:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39marray=\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    881\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mReshape your data either using array.reshape(-1, 1) if \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    882\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39myour data has a single feature or array.reshape(1, -1) \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    883\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mif it contains a single sample.\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(array)\n\u001b[0;32m    884\u001b[0m         )\n\u001b[0;32m    886\u001b[0m \u001b[39mif\u001b[39;00m dtype_numeric \u001b[39mand\u001b[39;00m array\u001b[39m.\u001b[39mdtype\u001b[39m.\u001b[39mkind \u001b[39min\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mUSV\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    887\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    888\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mdtype=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mnumeric\u001b[39m\u001b[39m'\u001b[39m\u001b[39m is not compatible with arrays of bytes/strings.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    889\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mConvert your data to numeric values explicitly instead.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    890\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Expected 2D array, got 1D array instead:\narray=[1449.  550. 1623. ... 2219. 1829. 2592.].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample."
     ]
    }
   ],
   "source": [
    "#Learn x-y relationships (principal components) and transform\n",
    "data_to_cluster_scaled = scaler.fit_transform(data['keywords_ID'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.Author_Keywords.value_counts().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.text_clean.value_counts().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['text_clean'].info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Categories'].info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fixing the imbalances and transforming the data into numerical values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le_categories = LabelEncoder()\n",
    "le_text_clean = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Categories_ID'] = le_categories.fit_transform(data['Categories'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['text_clean_ID'] = le_text_clean.fit_transform(data['text_clean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.Categories_ID.value_counts().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.text_clean_ID.value_counts().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[data['Categories_ID'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from numpy.random import RandomState\n",
    "from sklearn.datasets import make_classification\n",
    "from imblearn.over_sampling import SMOTENC\n",
    "SM = sm = SMOTENC(random_state=42, categorical_features=[18, 19])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['text_clean_ID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Categories_ID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we use oversampling, since the distribution of the y value is skewed \n",
    "X_train_SMOTEN, y_train_SMOTEN = sm.fit_resample(data['text_clean'], data['Categories'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset into the Training set and Test set (since we have a new output variable)\n",
    "X_train, X_test, y_train, y_test = train_test_split(data['text_clean'], data['Categories'], test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#instantiate models and \"bundle up as pipeline\"\n",
    "\n",
    "tfidf = TfidfVectorizer()\n",
    "cls = LogisticRegression()\n",
    "\n",
    "pipe = make_pipeline(tfidf, cls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.fit(X_train,y_train) # fit model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate model performance on training set\n",
    "\n",
    "y_eval = pipe.predict(X_train)\n",
    "report = classification_report(y_train, y_eval)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate model performance on test set\n",
    "\n",
    "y_pred = pipe.predict(X_test)\n",
    "report = classification_report(y_test, y_pred)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = ['']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1_p = text_prepro(pd.Series(t1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.predict (t1_p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = LdaMulticore(corpus, id2word=dictionary, num_topics=5, workers = 4, passes=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try to visualize\n",
    "lda_display = pyLDAvis.gensim_models.prepare(lda_model, corpus, dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Let's Visualize\n",
    "pyLDAvis.display(lda_display)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We will use a coherence matrix to find out how many topics we need for topic modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cm = CoherenceModel(model=lda_model, corpus=corpus, coherence='u_mass')\n",
    "# coherence = cm.get_coherence()  # get coherence value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[data[\"tokens\"].str.len() != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = data['tokens']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.tokens.value_counts().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirichlet_dict = corpora.Dictionary(corpus)\n",
    "bow_corpus = [dirichlet_dict.doc2bow(text) for text in corpus]\n",
    "\n",
    "# Considering 1-15 topics, as the last is cut off\n",
    "num_topics = list(range(10)[1:])\n",
    "num_keywords = 15\n",
    "\n",
    "LDA_models = {}\n",
    "LDA_topics = {}\n",
    "for i in num_topics:\n",
    "    LDA_models[i] = LdaModel(corpus=bow_corpus,\n",
    "                             id2word=dirichlet_dict,\n",
    "                             num_topics=i,\n",
    "                             update_every=1,\n",
    "                             chunksize=len(bow_corpus),\n",
    "                             passes=10,\n",
    "                             alpha='auto',\n",
    "                             random_state=42)\n",
    "\n",
    "    shown_topics = LDA_models[i].show_topics(num_topics=i, \n",
    "                                             num_words=num_keywords,\n",
    "                                             formatted=False)\n",
    "    LDA_topics[i] = [[word[0] for word in topic[1]] for topic in shown_topics]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_similarity(topic_1, topic_2):\n",
    "    \"\"\"\n",
    "    Derives the Jaccard similarity of two topics\n",
    "\n",
    "    Jaccard similarity:\n",
    "    - A statistic used for comparing the similarity and diversity of sample sets\n",
    "    - J(A,B) = (A ∩ B)/(A ∪ B)\n",
    "    - Goal is low Jaccard scores for coverage of the diverse elements\n",
    "    \"\"\"\n",
    "    intersection = set(topic_1).intersection(set(topic_2))\n",
    "    union = set(topic_1).union(set(topic_2))\n",
    "                    \n",
    "    return float(len(intersection))/float(len(union))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LDA_stability = {}\n",
    "for i in range(0, len(num_topics)-1):\n",
    "    jaccard_sims = []\n",
    "    for t1, topic1 in enumerate(LDA_topics[num_topics[i]]): # pylint: disable=unused-variable\n",
    "        sims = []\n",
    "        for t2, topic2 in enumerate(LDA_topics[num_topics[i+1]]): # pylint: disable=unused-variable\n",
    "            sims.append(jaccard_similarity(topic1, topic2))    \n",
    "        \n",
    "        jaccard_sims.append(sims)    \n",
    "    \n",
    "    LDA_stability[num_topics[i]] = jaccard_sims\n",
    "                \n",
    "mean_stabilities = [np.array(LDA_stability[i]).mean() for i in num_topics[:-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coherences = [CoherenceModel(model=LDA_models[i], texts=corpus, dictionary=dirichlet_dict, coherence='c_v').get_coherence() for i in num_topics[:-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_keywords = len(coherences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coh_sta_diffs = [coherences[i] - mean_stabilities[i] for i in range(num_keywords)[:-1]] # limit topic numbers to the number of keywords\n",
    "coh_sta_max = max(coh_sta_diffs)\n",
    "coh_sta_max_idxs = [i for i, j in enumerate(coh_sta_diffs) if j == coh_sta_max]\n",
    "ideal_topic_num_index = coh_sta_max_idxs[0] # choose less topics in case there's more than one max\n",
    "ideal_topic_num = num_topics[ideal_topic_num_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "ax = sns.lineplot(x=num_topics[:-1], y=mean_stabilities, label='Average Topic Overlap')\n",
    "ax = sns.lineplot(x=num_topics[:-1], y=coherences, label='Topic Coherence')\n",
    "\n",
    "ax.axvline(x=ideal_topic_num, label='Ideal Number of Topics', color='black')\n",
    "ax.axvspan(xmin=ideal_topic_num - 1, xmax=ideal_topic_num + 1, alpha=0.5, facecolor='grey')\n",
    "\n",
    "y_max = max(max(mean_stabilities), max(coherences)) + (0.10 * max(max(mean_stabilities), max(coherences)))\n",
    "ax.set_ylim([0, y_max])\n",
    "ax.set_xlim([1, num_topics[-1]-1])\n",
    "                \n",
    "ax.axes.set_title('Model Metrics per Number of Topics', fontsize=25)\n",
    "ax.set_ylabel('Metric Level', fontsize=20)\n",
    "ax.set_xlabel('Number of Topics', fontsize=20)\n",
    "plt.legend(fontsize=20)\n",
    "plt.show()  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "995406004189bc7a762cd15e0adc766b4032076a3e0cf164ca390850079b6fa1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
